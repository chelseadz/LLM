{"cells":[{"cell_type":"markdown","metadata":{"id":"RnOECYgu2jLg"},"source":["<center>\n","<h1>El mecanismo de atención a pie</h1>\n","\n","\n","<p> Julio Waissman Vilanova </p>\n","\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/atencion/atencion.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n","\n","Tomado parcialmente y adaptado de la libreta <i>C4_W1_Ungraded_Lab_1_Basic_Attention.ipynb</i> de la  <i>Especialización en procesamiento de lenguaje natural</i> de <i>Deeplearning.ai</i>, disponible en <i>Coursera</i>.\n","\n","</center>\n"]},{"cell_type":"markdown","metadata":{"id":"e9AeaFCv2jLw"},"source":["Como a estas alturas ya deberíamos haber visto en el curso, la atención permite que un modelo del tipo `seq2seq` utilice en cada paso del *decoder*, información de cada paso del *encoder* en lugar de solo el estado oculto final del codificador.\n","\n","En la operación de atención, las salidas del *encoder* se ponderan según el estado oculto del *decoder* y luego se combinan en un único vector de contexto. Este vector se utiliza luego como entrada para el decodificador y predecir el siguiente paso de salida.\n","\n","En esta libreta vamos a:\n","\n","- Implementar en numpy (a pie) el mecanismo de atención para ver como funciona (sin aprendizaje) tal como se propone en [el artículo original de Bhadanau, et al (2014)](https://arxiv.org/abs/1409.0473).\n","  \n","- Implementar en numpy (a pie) el mecanismo de atención tipo **QKV** (**Q**ueries, **K**eys, **V**alues) para ver como funciona (sin aprendizaje) tal como se propone en el famoso artículo de [Attention Is All You Need](https://arxiv.org/abs/1706.03762). Desde este artículo y basado en este mecanismo de atención, los transformadores han dominado a los modelos para procesamiento de lenguaje natural."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"fEt1rulO2jL1","executionInfo":{"status":"ok","timestamp":1715888368732,"user_tz":420,"elapsed":325,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}}},"outputs":[],"source":["import pickle\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"GGmwguFs2jL7"},"source":["Y la función *softmax* que no viene por default con numpy:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"zGymyewA2jL8","executionInfo":{"status":"ok","timestamp":1715888369119,"user_tz":420,"elapsed":29,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}}},"outputs":[],"source":["def softmax(x, axis=0):\n","    \"\"\" Calcula la función softmax en un eje específico\n","\n","        axis=0 calcula softmax en los renglones, cada columna de salida suma 1\n","        axis=1 calcula softmax en las columnas, cada renglón de salida suma 1\n","    \"\"\"\n","    return np.exp(x) / np.expand_dims(np.sum(np.exp(x), axis=axis), axis)"]},{"cell_type":"markdown","metadata":{"id":"rkS4orFT2jL9"},"source":["## El primer mecanismo de atención\n","\n","### El modelo\n","\n","El primer paso es calcular los puntajes de alineación. Esto es una medida de similitud entre el estado oculto del decodificador y cada estado oculto del codificador. Según el artículo, esta operación se ve así:\n","\n","$$\n","\\large e_{ij} = v_a^\\top \\tanh{\\left(W_a s_{i-1} + U_a h_j\\right)}\n","$$\n","\n","donde $W_a \\in \\mathbb{R}^{n\\times m}$, $U_a \\in \\mathbb{R}^{n \\times m} $ y $v_a \\in \\mathbb{R}^m$ son las matrices de peso y $n$ es el tamaño del estado oculto. En la práctica, esto se implementa como una red neuronal feedforward con dos capas, donde $m$ es el tamaño de las capas en la red de alineación. Se ve algo así:\n","\n","![alignment_model.png](attachment:alignment_model.png)\n","\n","Aquí, $h_j$ son los estados ocultos del codificador para cada paso de entrada $j$ y $s_{i - 1}$ es el estado oculto del decodificador del paso anterior. La primera capa corresponde a $W_a$ y $U_a$, mientras que la segunda capa corresponde a $v_a$.\n","\n","Para implementar esto, primero concatena los estados ocultos del codificador y del decodificador para producir un array con tamaño $K \\times 2n$, donde $K$ es el número de estados/pasos del codificador. Para esto, utiliza `np.concatenate` ([documentación](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)).\n","\n","Ten en cuenta que solo hay un estado del decodificador, así que necesitarás remodelarlo para concatenar los arrays correctamente. La forma más fácil es usar `decoder_state.repeat` ([documentación](https://numpy.org/doc/stable/reference/generated/numpy.repeat.html#numpy.repeat)) para que coincida con el tamaño del array de estados ocultos.\n","\n","Luego, aplica la primera capa como una multiplicación de matrices entre los pesos y la entrada concatenada. Usa la función tanh para obtener las activaciones. Finalmente, realiza la multiplicación de matrices de los pesos de la segunda capa y las activaciones. Esto te dará los puntajes de alineación."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"v--ZRrCf2jL_","executionInfo":{"status":"ok","timestamp":1715888369120,"user_tz":420,"elapsed":27,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}}},"outputs":[],"source":["hidden_size = 16\n","attention_size = 10\n","input_length = 5\n","\n","# Ejemplos de dalva para probar\n","np.random.seed(42)\n","encoder_states = np.random.randn(input_length, hidden_size)\n","decoder_state = np.random.randn(1, hidden_size)\n","\n","# Pesos usados en el mecanismo de atención. Estos se aprenden en el proceso de\n","# aprendizaje, pero aqui vaos a ponerlos como pesos fijos en forma pseudoaleatoria\n","layer_1 = np.random.randn(2 * hidden_size, attention_size)\n","layer_2 = np.random.randn(attention_size, 1)\n","\n","# La función de alineamiento\n","# (para encontrar los pesos de las entradas en las salidas)\n","\n","def alignment(encoder_states, decoder_state):\n","\n","    # Concatena las dos entradas\n","    inputs = np.concatenate(\n","        (encoder_states, decoder_state.repeat(input_length, axis=0)),\n","        axis=1\n","    )\n","\n","    # Capa densa con activación tanh\n","    activations = np.tanh(inputs @ layer_1)\n","\n","    # Capa densa con activación lineal\n","    scores = activations @ layer_2\n","\n","    # fin de la red neuronal de atención\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"BvBCQUSP2jMA"},"source":["y para probar..."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXkMgFxe2jMB","executionInfo":{"status":"ok","timestamp":1715888369121,"user_tz":420,"elapsed":26,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}},"outputId":"e47ae494-1c13-43c0-d99b-84fa394634ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[4.35790943]\n"," [5.92373433]\n"," [4.18673175]\n"," [2.11437202]\n"," [0.95767155]]\n"]}],"source":["scores = alignment(encoder_states, decoder_state)\n","print(scores)"]},{"cell_type":"markdown","metadata":{"id":"oiYymcjw2jMD"},"source":["Deberíamos tener en la salida anterior:\n","\n","```python\n","[[4.35790943]\n"," [5.92373433]\n"," [4.18673175]\n"," [2.11437202]\n"," [0.95767155]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"o6tp4AIj2jMF"},"source":["### Ponderar los vectores de salida del codificador y sumar\n","\n","El siguiente paso es calcular los pesos a partir de los puntajes de alineación. Estos pesos determinan las salidas del codificador que son más importantes para la salida del decodificador. Estos pesos deben estar entre 0 y 1, y sumar 1.\n","\n","Para esto vamos a usar la función softmax que ya he implementado arriba. Los pesos te indican la importancia de cada palabra de entrada en relación con el estado del decodificador. En este paso, utilizas los pesos para modular la magnitud de los vectores del codificador.\n","\n","Las palabras con poca importancia se reducirán en comparación con las palabras importantes. Multiplica cada vector del codificador por su respectivo peso para obtener los vectores de alineación y luego suma los vectores de alineación ponderados para obtener el vector de contexto. Matemáticamente,\n","\n","$$\n","\\large c_i = \\sum_{j=1}^K\\alpha_{ij} h_{j}\n","$$"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRJwiL1W2jMI","executionInfo":{"status":"ok","timestamp":1715888369123,"user_tz":420,"elapsed":23,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}},"outputId":"4adca063-9e72-4ea8-b8e7-bebf77c9bc5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n","  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n"," -0.58015282 -0.58294027 -0.75457577  1.32985756]\n"]}],"source":["def attention(encoder_states, decoder_state):\n","    \"\"\" Example function that calculates attention, returns the context vector\n","\n","        Arguments:\n","        encoder_vectors: NxM numpy array, where N is the number of vectors and M is the vector length\n","        decoder_vector: 1xM numpy array, M is the vector length, much be the same M as encoder_vectors\n","    \"\"\"\n","\n","    # Calcula el score de alignment\n","    scores = alignment(encoder_states, decoder_state)\n","\n","    # Softmax\n","    weights = softmax(scores)\n","\n","    # Multiplica cada estado del encoder por su peso respectivo\n","    weighted_scores = encoder_states * weights\n","\n","    # Suma los pesos por los ejemplos para encontrar la salida\n","    context = weighted_scores.sum(axis=0)\n","    return context\n","\n","context_vector = attention(encoder_states, decoder_state)\n","print(context_vector)"]},{"cell_type":"markdown","metadata":{"id":"RNiWgUj32jML"},"source":["Si la función de `attention` se implementó correctamente, el vector de contexto debería ser:\n","\n","```python\n","[-0.63514569  0.04917298 -0.43930867 -0.9268003   1.01903919 -0.43181409\n","  0.13365099 -0.84746874 -0.37572203  0.18279832 -0.90452701  0.17872958\n"," -0.58015282 -0.58294027 -0.75457577  1.32985756]\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9Nf3tIuq2jMN"},"source":["## El mecanismo de atención tipo QKV\n","\n","### Obteniendo los datos para el ejemplo\n","\n","El modelo  aprende cómo alinear palabras en diferentes idiomas. No se entrenarán pesos aquí, así que en su lugar vamos a usear embeddings de palabras alineadas pre-entrenadas cono las de [fasttext](https://fasttext.cc/docs/en/aligned-vectors.html).\n","\n","La siguientes celdas lo hacen a pie, pero yo te recomiendo que saltes estas primeras celdas sn ejecutar (tardan muchisimo) y utilices los archivos que obtuvimos con estas celdas:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"paWG0h3p2jMO","executionInfo":{"status":"ok","timestamp":1715888422220,"user_tz":420,"elapsed":53114,"user":{"displayName":"Chelsea Durazo","userId":"07901205626479946572"}},"outputId":"e9e8ded8-d1dd-4f90-fcbf-4f8c49daef1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2124M  100 2124M    0     0   127M      0  0:00:16  0:00:16 --:--:--  186M\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 5422M  100 5422M    0     0   151M      0  0:00:35  0:00:35 --:--:--  136M\n"]}],"source":["!curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.es.align.vec\n","!curl -O https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.en.align.vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9Yubk_X2jMQ"},"outputs":[],"source":["en_words = {}\n","en_emb = None\n","\n","with open(\"wiki.en.align.vec\") as fp:\n","  i = 0\n","  for line in fp.readlines():\n","    line_l = line.split(' ')\n","    if i == 0:\n","      en_emb = np.zeros((int(line_l[0]), int(line_l[1])))\n","    else:\n","      en_words[line_l[0]] = i - 1\n","      en_emb[i - 1, :] = np.array([float(d) for d in line_l[1:]])\n","    i += 1\n","\n","print(len(en_words))\n","print(en_emb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXBTvrEY2jMR"},"outputs":[],"source":["es_words = {}\n","es_emb = None\n","\n","with open(\"wiki.es.align.vec\") as fp:\n","  i = 0\n","  for line in fp.readlines():\n","    line_l = line.split(' ')\n","    if i == 0:\n","      es_emb = np.zeros((int(line_l[0]), int(line_l[1])))\n","    else:\n","      es_words[line_l[0]] = i - 1\n","      es_emb[i - 1, :] = np.array([float(d) for d in line_l[1:]])\n","    i += 1\n","\n","print(len(es_words))\n","print(es_emb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPfXk63D2jMS"},"outputs":[],"source":["np.save(\"en_emb\", en_emb)\n","np.save(\"es_emb\", es_emb)\n","\n","with open('en_words.pickle', 'wb') as fp:\n","    pickle.dump(en_words, fp, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('es_words.pickle', 'wb') as fp:\n","    pickle.dump(es_words, fp, protocol=pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{"id":"v9kPCmi82jMU"},"source":["Para evitar ejecutar esto (no corre en colab si no tienes la versión pro, por falta de RAM), puede descargar directamente los archivos ya transformados [desde mi espacio de *One Drive* de la Universidad de Sonora](https://unisonmx-my.sharepoint.com/:f:/g/personal/julio_waissman_unison_mx/EhtLxLcVuZFHizfT3bFR8zABKw7Lxz3Bow8JtoRwl1NtQA?e=C6Bx6L).\n","\n","No hay manera sencilla de automatizar la descarga de *One Drive*, así que los tendrás que bajar a mano y ponerlos en el mismo repositorio donde estás ejecutando la libreta. Con todo es la forma más rápida.\n","\n","Si tienes los archivos `npz` y `pickle` correspondientes, entonces simplemente:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M85QvRrZ2jMV"},"outputs":[],"source":["# Carga los diccionarios word2idxs\n","with open('en_words.pickle', 'rb') as fp:\n","    en_words = pickle.load(fp)\n","\n","with open('es_words.pickle', 'rb') as fp:\n","    es_words = pickle.load(fp)\n","\n","with open('en_emb.npy', 'rb') as fp:\n","    en_emb = np.load(fp)\n","\n","with open('es_emb.npy', 'rb') as fp:\n","    es_emb = np.load(fp)\n","\n","print(f\"La forma de en_emb es un ndarray de forma {en_emb.shape}\")\n","print(f\"La forma de es_emb es un ndarray de forma {es_emb.shape}\")\n","print(f\"es_words es un diccionario de dimensión {len(es_emb)} cuyas claves son indices y los valores tokens\")\n","print(f\"es_words es un diccionario de dimensión {len(en_emb)} cuyas claves son indices y los valores tokens\")\n"]},{"cell_type":"markdown","metadata":{"id":"d-d_adId2jMW"},"source":["### Preprocesamiento de datos\n","\n","Vamos a usar una función para convertir una sentencia a una lista de tokens (palabras), y otra para convertir los tokens en embeddings, de acuerdo al embedding e indice elegido."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYVsOHaS2jMX"},"outputs":[],"source":["def tokenize(sentence, token_mapping):\n","    tokenized = []\n","\n","    for word in sentence.lower().split(\" \"):\n","        tokenized.append(token_mapping.get(word, -1))\n","\n","    return tokenized\n","\n","def embed(tokens, embeddings):\n","    embed_size = embeddings.shape[1]\n","\n","    output = np.zeros((len(tokens), embed_size))\n","    for i, token in enumerate(tokens):\n","        if token == -1:\n","            output[i] = np.zeros((1, embed_size))\n","        else:\n","            output[i] = embeddings[token]\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"4PZKbgfb2jMY"},"source":["### El mecanismo de atención tipo QKV\n","\n","La atención por producto punto escalado consiste en dos multiplicaciones de matrices y una escala softmax, como se muestra en el diagrama a continuación de [Vaswani, et al. (2017)](https://arxiv.org/abs/1706.03762). Toma tres matrices de entrada: las consultas (queries), las claves (keys) y los valores (values).\n","\n","![attention.png](attachment:attention.png)\n","\n","Matemáticamente, esto se expresa como\n","\n","$$\n","\\large \\mathrm{Atención}\\left(Q, K, V\\right) = \\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V\n","$$\n","\n","donde $Q$, $K$ y $V$ son las matrices de consultas (query), claves (key) y valores (values) respectivamente, y $d_k$ es la dimensión de las claves. En la práctica, Q, K y V tienen todas las mismas dimensiones. Esta forma de atención es más rápida y eficiente en espacio, ya que consta solo de multiplicaciones de matrices en lugar de una capa de alimentación adelante aprendida.\n","\n","Conceptualmente, la primera multiplicación de matrices es una medida de la similitud entre las consultas y las claves. Esto se transforma en pesos usando la función softmax. Estos pesos luego se aplican a los valores con la segunda multiplicación de matrices, lo que da como resultado vectores de atención de salida.\n","\n","Típicamente, los estados del decodificador se utilizan como consultas mientras que los estados del codificador son las claves y los valores.\n","\n","Para ilustrar como funciona, vamos a calcular las relaciones de pesos entre secuencia de encoder y decoder (relación entre consultas y llaves) antes de calcular los pesos:  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5iGN8vp2jMa"},"outputs":[],"source":["def calculate_weights(queries, keys):\n","    \"\"\" Calculate the weights for scaled dot-product attention\"\"\"\n","    # Replace None with your code.\n","    dot = queries @ keys.T / np.sqrt(keys.shape[1])\n","    weights = softmax(dot, axis=1)\n","\n","    return weights"]},{"cell_type":"markdown","metadata":{"id":"SLtB6mzY2jMb"},"source":["Y vamos probando con un ejemplo la relacion entre la secuencia de entrada al encoder y de salida del decoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrcJiaV22jMc"},"outputs":[],"source":["sentence_en = \"The agreement on the European Economic Area was signed in August 1992 .\"\n","tokenized_en = tokenize(sentence_en, en_words)\n","embedded_en = embed(tokenized_en, en_emb)\n","\n","sentence_es = \"El acuerdo sobre la zona económica europea fue firmada en agosto 1992 .\"\n","tokenized_es = tokenize(sentence_es, es_words)\n","embedded_es = embed(tokenized_es, es_emb)\n","\n","# These weights indicate alignment between words in English and French\n","alignment = calculate_weights(embedded_es, embedded_en)\n","\n","# Visualize weights to check for alignment\n","fig, ax = plt.subplots(figsize=(7,7))\n","ax.imshow(alignment, cmap='gray')\n","ax.xaxis.tick_top()\n","ax.set_xticks(np.arange(alignment.shape[1]))\n","ax.set_xticklabels(sentence_en.split(\" \"), rotation=90, size=16);\n","ax.set_yticks(np.arange(alignment.shape[0]));\n","ax.set_yticklabels(sentence_es.split(\" \"), size=16);"]},{"cell_type":"markdown","metadata":{"id":"uyp316xh2jMe"},"source":["Y ahora si podemos ver como se reparten los pesos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fo99IJdW2jMf"},"outputs":[],"source":["def attention_qkv(queries, keys, values):\n","    return calculate_weights(queries, keys) @ values\n","\n","\n","attention_qkv_result = attention_qkv(embedded_es, embedded_en, embedded_en)\n","\n","print(f\"La dimensión de los pesos obtenidos con attention_qkv es: {attention_qkv_result.shape}\")\n","\n","print(f\"Algunos elementos de la salida de pesos son:\\n{attention_qkv_result[0:2,:10]}\")"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}